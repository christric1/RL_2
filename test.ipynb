{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check pytorch,cuda & cudnn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.cudnn.is_available())\n",
    "print(torch.backends.cudnn.version()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "actual = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "Precision = tp/(tp+fp)\n",
    "Recall= tp/(tp+fn)\n",
    "print(Accuracy, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 2 / ( (1/ Precision) + (1/ Recall) )\n",
    "print(F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from reinforcement import modify_image\n",
    "\n",
    "trainDataset = OD_Dataset(\"Pascal_2012\")\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(trainDataloader))\n",
    "labels, boxs = target[\"labels\"].squeeze(dim=0), target[\"boxes\"].squeeze(dim=0)\n",
    "image = data.squeeze(dim=0)\n",
    "pil_img_0 = TF.to_pil_image(image)\n",
    "# plt.imshow(pil_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_img = TF.adjust_brightness(image, brightness_factor=2.0)\n",
    "pil_img_1 = TF.to_pil_image(bright_img)\n",
    "\n",
    "saturation_img = TF.adjust_saturation(image, saturation_factor=2.0)\n",
    "pil_img_2 = TF.to_pil_image(saturation_img)\n",
    "\n",
    "contrast_img = TF.adjust_contrast(image, contrast_factor=2.0)\n",
    "pil_img_3 = TF.to_pil_image(contrast_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TF.adjust_saturation(image, saturation_factor=1.5)\n",
    "b = TF.adjust_contrast(a, contrast_factor=1.5)\n",
    "c = TF.adjust_brightness(b, brightness_factor=1.5)\n",
    "result_img = TF.to_pil_image(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(15,5))\n",
    "\n",
    "axs[0].imshow(pil_img_0)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(pil_img_1)\n",
    "axs[1].set_title('Brightness Adjustment')\n",
    "axs[2].imshow(pil_img_2)\n",
    "axs[2].set_title('Saturation Adjustment')\n",
    "axs[3].imshow(pil_img_3)\n",
    "axs[3].set_title('Contrast Adjustment')\n",
    "axs[4].imshow(result_img)\n",
    "axs[4].set_title('result_img')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforcement import distortion_image\n",
    "t = distortion_image(image)\n",
    "t_img = TF.to_pil_image(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "axs[0].imshow(pil_img_0)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(t_img)\n",
    "axs[1].set_title('Result Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):\n",
    "    '''\n",
    "        bbox : (xmin, ymin, xmax, ymax)\n",
    "        color: blue, red, etc\n",
    "    '''\n",
    "    return plt.Rectangle(\n",
    "        xy=(bbox[0], bbox[1]),      \n",
    "        width=bbox[2]-bbox[0],      \n",
    "        height=bbox[3]-bbox[1],     \n",
    "        fill=False,             \n",
    "        edgecolor=color,\n",
    "        linewidth=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawImageWithBox(image, labels, boxes):\n",
    "\n",
    "    def get_classes(classes_path):\n",
    "        import yaml\n",
    "        with open(classes_path, 'r') as f:\n",
    "            params = yaml.safe_load(f)\n",
    "        names = params['names']\n",
    "        count = len(names)\n",
    "        return names, count\n",
    "    \n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import colorsys\n",
    "    class_names, num_classes = get_classes(\"coco/data.yaml\")\n",
    "    font        = ImageFont.truetype(font='models/simhei.ttf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "    thickness   = int(max((image.size[0] + image.size[1]) // np.mean([640, 640]), 1))\n",
    "\n",
    "    hsv_tuples = [(x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    if isinstance(boxes, torch.Tensor):\n",
    "        boxes = boxes.numpy()\n",
    "\n",
    "    for i, c in list(enumerate(labels)):\n",
    "        predicted_class = class_names[int(c)]\n",
    "        box             = boxes[i]\n",
    "\n",
    "        # (xmin, ymin, xmax, ymax) -> (ymin, xmin, ymax, xmax)\n",
    "        box[[1, 0, 3, 2]] = box[[0, 1, 2, 3]]\n",
    "        top, left, bottom, right = box\n",
    "\n",
    "        top     = max(0, np.floor(top).astype('int32'))\n",
    "        left    = max(0, np.floor(left).astype('int32'))\n",
    "        bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n",
    "        right   = min(image.size[0], np.floor(right).astype('int32'))\n",
    "\n",
    "        label = '{}'.format(predicted_class)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # 使用 textbbox 來計算 label 的大小\n",
    "        label_bbox = draw.textbbox((0, 0), label, font)\n",
    "        label_width = label_bbox[2] - label_bbox[0]\n",
    "        label_height = label_bbox[3] - label_bbox[1]\n",
    "        \n",
    "        label = label.encode('utf-8')\n",
    "        \n",
    "        if top - label_height >= 0:\n",
    "            text_origin = np.array([left, top - label_height])\n",
    "        else:\n",
    "            text_origin = np.array([left, top + 1])\n",
    "\n",
    "        for i in range(thickness):\n",
    "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
    "        draw.rectangle([tuple(text_origin), tuple(text_origin + (label_width, label_height))], fill=colors[c])\n",
    "        draw.text(text_origin, str(label,'UTF-8'), fill=(0, 0, 0), font=font)\n",
    "        del draw\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from reinforcement import modify_image\n",
    "\n",
    "trainDataset = OD_Dataset(\"coco\")\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=1, shuffle=True)\n",
    "\n",
    "from yolo import YOLO\n",
    "yolo = YOLO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(trainDataloader))\n",
    "labels, boxs = target[\"labels\"].squeeze(dim=0), target[\"boxes\"].squeeze(dim=0)\n",
    "image = data.squeeze(dim=0)\n",
    "pil_img_0 = TF.to_pil_image(image)\n",
    "result_image = yolo.detect_image(pil_img_0.copy())\n",
    "groundTrue_image = drawImageWithBox(pil_img_0.copy(), labels, boxs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].imshow(result_image)\n",
    "axs[0].set_title('result_image')\n",
    "axs[1].imshow(groundTrue_image)\n",
    "axs[1].set_title('groundTrue_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.detectImg(image, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import LoadImages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "dataset = LoadImages(\"inference/images/zidane.jpg\", img_size=640, stride=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, img, im0s, vid_cap = next(iter(dataset))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(cv2.cvtColor(im0s, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(img.transpose(1, 2, 0))\n",
    "axs[1].set_title('modify Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    import numpy as np\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset import OD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "trainDataset = OD_Dataset(\"coco\")\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = next(iter(trainDataloader))\n",
    "img_ = img.squeeze(dim=0).permute(1, 2, 0).numpy()\n",
    "img_ = (img_[:, :, ::-1] * 255).astype(np.uint8)\n",
    "\n",
    "result = letterbox(img_, 640, stride=32)[0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(TF.to_pil_image(img[0]))\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('modify Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawImageWithBox(image, labels, boxes):\n",
    "\n",
    "    def get_classes(classes_path):\n",
    "        import yaml\n",
    "        with open(classes_path, 'r') as f:\n",
    "            params = yaml.safe_load(f)\n",
    "        names = params['names']\n",
    "        count = len(names)\n",
    "        return names, count\n",
    "    \n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import colorsys\n",
    "    class_names, num_classes = get_classes(\"coco/data.yaml\")\n",
    "    font        = ImageFont.truetype(font='models/simhei.ttf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "    thickness   = int(max((image.size[0] + image.size[1]) // np.mean([640, 640]), 1))\n",
    "\n",
    "    hsv_tuples = [(x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    if isinstance(boxes, torch.Tensor):\n",
    "        boxes = boxes.numpy()\n",
    "\n",
    "    for i, c in list(enumerate(labels)):\n",
    "        predicted_class = class_names[int(c)]\n",
    "        box             = boxes[i]\n",
    "\n",
    "        # (xmin, ymin, xmax, ymax) -> (ymin, xmin, ymax, xmax)\n",
    "        box[[1, 0, 3, 2]] = box[[0, 1, 2, 3]]\n",
    "        top, left, bottom, right = box\n",
    "\n",
    "        top     = max(0, np.floor(top).astype('int32'))\n",
    "        left    = max(0, np.floor(left).astype('int32'))\n",
    "        bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n",
    "        right   = min(image.size[0], np.floor(right).astype('int32'))\n",
    "\n",
    "        label = '{}'.format(predicted_class)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # 使用 textbbox 來計算 label 的大小\n",
    "        label_bbox = draw.textbbox((0, 0), label, font)\n",
    "        label_width = label_bbox[2] - label_bbox[0]\n",
    "        label_height = label_bbox[3] - label_bbox[1]\n",
    "        \n",
    "        label = label.encode('utf-8')\n",
    "        \n",
    "        if top - label_height >= 0:\n",
    "            text_origin = np.array([left, top - label_height])\n",
    "        else:\n",
    "            text_origin = np.array([left, top + 1])\n",
    "\n",
    "        for i in range(thickness):\n",
    "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
    "        draw.rectangle([tuple(text_origin), tuple(text_origin + (label_width, label_height))], fill=colors[c])\n",
    "        draw.text(text_origin, str(label,'UTF-8'), fill=(0, 0, 0), font=font)\n",
    "        del draw\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset import OD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from detect import yolo\n",
    "\n",
    "trainDataset = OD_Dataset(\"Pascal_2012\")\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=1, shuffle=True)\n",
    "model = yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, image_path = next(iter(trainDataloader))\n",
    "# img = img.squeeze(dim=0)\n",
    "# target = target.squeeze(dim=0)\n",
    "# labels, boxs = target[:, 0], target[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(boxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4301004542424461, 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# img = Image.open(\"inference/images/bus.jpg\")\n",
    "# img = TF.to_tensor(img).squeeze(dim=0)\n",
    "model.detectImg(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --source {image_path[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "names2 = ['airplane', 'apple', 'backpack', 'banana', 'baseball bat', 'baseball glove', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'cell phone', 'chair', 'clock', 'cow', 'cup', 'dining table', 'dog', 'donut', 'elephant', 'fire hydrant', 'fork', 'frisbee', 'giraffe', 'hair drier', 'handbag', 'horse', 'hot dog', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'parking meter', 'person', 'pizza', 'potted plant', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'couch', 'spoon', 'sports ball', 'stop sign', 'suitcase', 'surfboard', 'teddy bear', 'tennis racket', 'tie', 'toaster', 'toilet', 'toothbrush', 'traffic light', 'train', 'truck', 'tv', 'umbrella', 'vase', 'wine glass', 'zebra']\n",
    "map_index = [names2.index(i) for i in names1]\n",
    "\n",
    "from utils.general import scale_boxs\n",
    "scale_boxs(boxs, img.shape[1:])\n",
    "drawImageWithBox(TF.to_pil_image(img), torch.tensor([map_index[i] for i in labels.to(torch.int)]), boxs.to(torch.int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入預訓練的 ResNet18 模型\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# 移除全連接層 (fc)\n",
    "resnet18_backbone = torch.nn.Sequential(*(list(resnet18.children())[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 14, 14])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_backbone(torch.randn(1, 3, 448, 448)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_backbone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
